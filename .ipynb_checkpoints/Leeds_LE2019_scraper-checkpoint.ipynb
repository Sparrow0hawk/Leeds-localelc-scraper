{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text scrape Leeds City Council Election results into csv \n",
    "\n",
    "Data will be published on Leeds data mill but if you need it sooner than two weeks you can use the following code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for querying website\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of links for scrapping\n",
    "\n",
    "main_page = 'https://www.leeds.gov.uk/your-council/elections/leeds-city-council-election-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up selenium to use chrome in headless state\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "wd = webdriver.Chrome('/Users/alexcoleman/Downloads/chromedriver', options=options)\n",
    "\n",
    "wd.get(main_page)\n",
    "\n",
    "html_page = wd.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# opens page from selenium in beautiful soup\n",
    "import re \n",
    "\n",
    "soup = BeautifulSoup(html_page, 'lxml')\n",
    "\n",
    "# finds specific section of page with links to each ward\n",
    "link_list = soup.find(id='ctl00_ctl46_g_5037fcd1_1720_4395_8f61_756022543ee2')\n",
    "\n",
    "final_list_links = []\n",
    "\n",
    "# get all the links from specific section and append to list\n",
    "for link in link_list.find_all('a'):\n",
    "    final_list_links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format whitespace in links\n",
    "final_list_links = [lnk.replace(' ','%20') for lnk in final_list_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.leeds.gov.uk/your-council/elections/leeds-city-council-election-results?ward=Adel%20and%20Wharfedale\n"
     ]
    }
   ],
   "source": [
    "# check formatting is correct\n",
    "for link in final_list_links[0:1]:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example extracting election results from a page\n",
    "    \n",
    "df_y = []\n",
    "\n",
    "for link in final_list_links:\n",
    "    \n",
    "    # open webpage and get beautiful soup via selenium\n",
    "    \n",
    "    wd.get(link)\n",
    "\n",
    "    html_page = wd.page_source\n",
    "    \n",
    "    # get page into bs\n",
    "    soup = BeautifulSoup(html_page, 'lxml')\n",
    "\n",
    "    # extract data table\n",
    "    \n",
    "    df = pd.read_html(html_page,header=0)[1]\n",
    "    \n",
    "    df['Share'] = round(((df['Votes'] / df['Votes'].sum())*100),1)\n",
    "    \n",
    "    # extract top row data on ward, turnout and electorate size\n",
    "    top_row = soup.find(id=\"WebPartctl00_ctl46_g_5037fcd1_1720_4395_8f61_756022543ee2\")\n",
    "    \n",
    "    top_row = top_row.text.strip()\n",
    "    top_row = top_row[46:125]\n",
    "    top_row = top_row.split(\" \")\n",
    "    \n",
    "    # get turnout\n",
    "    for i in top_row:\n",
    "        if i == \"%\":\n",
    "            break\n",
    "        else:\n",
    "            z = ''\n",
    "            z += i\n",
    "        \n",
    "        df[\"Turnout\"] = z\n",
    "        j = \"\"\n",
    "        # get total ward\n",
    "        \n",
    "    for i in top_row:\n",
    "        if i == \"Electorate\":\n",
    "            break\n",
    "        else:\n",
    "            j += \" \"+i\n",
    "            df['Ward'] = j\n",
    "\n",
    "        # get total electorate\n",
    "    for i in top_row:\n",
    "        if i == \"Turnout\":  \n",
    "            break\n",
    "        else:\n",
    "            b = \"\"\n",
    "            b += i\n",
    "    df['Electorate'] = b\n",
    "                \n",
    "    # append dataframe to list of dataframes\n",
    "    df_y.append(df)\n",
    "    \n",
    "df_y = pd.concat(df_y,axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.to_csv(\"./Leeds_LE2019_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
