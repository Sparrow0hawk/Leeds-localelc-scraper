{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text scrape Leeds City Council Election results into csv \n",
    "\n",
    "Data will be published on Leeds data mill but if you need it sooner than two weeks you can use the following code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for querying website\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since 2022 Leeds city council has listed results on a single page on their website\n",
    "# we set the address as a variable below\n",
    "\n",
    "main_page = 'https://www.leeds.gov.uk/your-council/elections/leeds-city-council-election-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up selenium to use chrome in headless state\n",
    "# this opens the page using chrome in a selenium session\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "wd.get(main_page)\n",
    "\n",
    "html_page = wd.page_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# opens page from selenium in beautiful soup\n",
    "import re \n",
    "\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# the page is organised in a series of accordion html elements\n",
    "# we use beautiful soup to select out the main div containing these accordion elements\n",
    "accordion_layer = soup.find(id=re.compile(r\"acc_[0-9]{4}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each result is contained in it's own accordion section with a section containing the ward name \n",
    "# and a section with the main results table\n",
    "# here we select out all the accordion id names as a python list\n",
    "full_accordion_id_names = [tag['id'] for tag in accordion_layer.findAll(True, {'id':True})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main section doing the scraping\n",
    "# the logic here is to iterate through the accordion id names\n",
    "# find it in the beautifulsoup object\n",
    "# if the id name contains trigger its content is just the ward name, assign this to a variable\n",
    "# for all other id names\n",
    "# read the html using pandas, prettifying the beautifulsoup object to allow pandas to read it correctly\n",
    "# this returns a list of 2 dataframes, 1 the main results data, 2 a table with turnout, spoilt ballots and electorate\n",
    "# do some logic on this to add it as columns to the results data and append it to our frame stack list\n",
    "# at the end concatenate all these small dataframes together into one dataframe\n",
    "\n",
    "frame_stack = []\n",
    "\n",
    "for layer in full_accordion_id_names:\n",
    "\n",
    "    if \"trigger\" in layer:\n",
    "        \n",
    "        ward = accordion_layer.find(id=layer).contents[0].strip()\n",
    "\n",
    "    else:\n",
    "        tbl_list = pd.read_html(accordion_layer.find(id=layer).prettify())\n",
    "\n",
    "        # transpose the metadata table \n",
    "        meta_tbl = tbl_list[1].T\n",
    "\n",
    "        main_tbl = tbl_list[0]\n",
    "\n",
    "        # set the columns of the metadata table to the first row\n",
    "        # as pandas weirdly misreads this table\n",
    "        meta_tbl.columns = meta_tbl.loc[0,:]\n",
    "\n",
    "        # remove the colon in these column names\n",
    "        meta_tbl.columns = meta_tbl.columns.str.replace(\":\",\"\")\n",
    "\n",
    "        # drop the row used to assign column names\n",
    "        meta_tbl.drop(0, axis=0, inplace=True)\n",
    "\n",
    "        # for each column in the metadata table \n",
    "        # add it to the main data table as a new row where every value is the \n",
    "        # single value in the metadata table\n",
    "        for col in meta_tbl.columns:\n",
    "            main_tbl[col] = meta_tbl[col].values[0]\n",
    "\n",
    "        main_tbl['Ward'] = ward\n",
    "\n",
    "        # create vote share column\n",
    "        main_tbl['vote_share'] = round(main_tbl.Votes / main_tbl.Votes.sum() * 100, 1)\n",
    "\n",
    "        frame_stack.append(main_tbl)\n",
    "\n",
    "results_frame = pd.concat(frame_stack)\n",
    "\n",
    "# convert turnout column to float\n",
    "results_frame.Turnout = results_frame.Turnout.str.replace(\"%\",'').astype(float)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the webdriver\n",
    "wd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frame.to_csv(\"../data/Leeds_LE2023_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
